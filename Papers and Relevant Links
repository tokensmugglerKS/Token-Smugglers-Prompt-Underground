2025

1. One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs
Yu Li et al., ICLR 2025
PDF: https://arxiv.org/pdf/2505.17598
GitHub: https://github.com/LLBao/ArrAttack


2. Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation
Xiang Li et al., May 2025
PDF: https://arxiv.org/abs/2506.17231


3. The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks
Berezin et al., Jan 2025
PDF: https://arxiv.org/abs/2501.18626


4. Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing
Wahréus et al., Mar 2025
PDF: https://arxiv.org/abs/2503.21598


5. Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails
Anonymous, Apr 2025
PDF: https://arxiv.org/html/2504.11168v1


6. SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Prompt Compression
Li, Ahn, Jiang et al., Jun 2025
PDF: https://arxiv.org/abs/2506.12707
GitHub: https://aka.ms/SecurityLingua


7. BitBypass: Black-Box Jailbreak with Bitstream Camouflage
Unknown authors, Jun 2025
PDF: https://arxiv.org/html/2506.02479v1


8. Graph-of-Attacks (GoAT): Black-Box Interpretable Jailbreaks
Akbar-Tajari et al., Apr 2025
PDF: https://arxiv.org/abs/2504.19019


9. Policy Puppetry Blog
HiddenLayer Research, Mar 2025
Blog: https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/


10. FlipAttack Prompt Injection
Keysight Technologies, May 2025
Blog: https://www.keysight.com/blogs/en/tech/nwvs/2025/05/20/prompt-injection-techniques-jailbreaking-large-language-models-via-flipattack


11. Invisible Character and Emoji Jailbreak Obfuscation
Mindgard.ai Blog, May 2025
Blog: https://mindgard.ai/blog/outsmarting-ai-guardrails-with-invisible-characters-and-adversarial-prompts


12. JailbreakBench Project
Community project, ongoing 2025
GitHub: https://github.com/JailbreakBench/jailbreakbench


13. Skeleton Key: A New Multi-turn Jailbreak Strategy
Mark Russinovich, Microsoft, June 2025
Blog: https://techcommunity.microsoft.com/t5/security-compliance-and-identity/mitigating-skeleton-key-a-new-type-of-generative-ai-jailbreak/ba-p/4133563


14. Many-shot Jailbreaking
Anthropic AI Research, April 2025
Blog: https://www.anthropic.com/index/many-shot-jailbreaking


15. Automated Multi-Turn Jailbreaks
AIM Intelligence, 2025
GitHub: https://github.com/AIM-Intelligence/Automated-Multi-Turn-Jailbreaks


16. Red Teaming LLMs via UCAR, Tom & Jerry, Linguistic Bypass
Adversa AI, March 2025
Blog: https://adversa.ai/blog/red-teaming-llms


17. How I Jailbroke GPT-4, GPT-4 Mini, Claude, and Gemini 1.5 Pro
Venkata Sai Santosh, Medium, Sep 2025
Blog: https://medium.com/@venkatasai/how-i-jailbroke-gpt-4-gemini-claude-with-a-single-prompt-2025


18. How to Jailbreak Mistral AI LLMs
Anakin AI Blog, 2025
Blog: https://anakin.ai/blog/how-to-jailbreak-mistral-llm
GitHub: https://github.com/AnakinAI/jailbreak-prompts

2024

19. ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings
Hao Wang et al., EMNLP 2024
PDF: https://aclanthology.org/2024.emnlp-main.157.pdf


20. Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
Maksym Andriushchenko et al., April 2, 2024
PDF: https://arxiv.org/abs/2404.02151
GitHub: https://github.com/tml-epfl/llm-adaptive-attacks


21. DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak
Hao Wang et al., Dec 2024
PDF: https://arxiv.org/abs/2412.17522


22. GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs
Advik Raj Basani, Xiao Zhang; Nov 21, 2024
PDF: https://arxiv.org/abs/2411.14133


23. Iterative Refinement Induced Self-Jailbreak (IRIS)
Govind Ramesh et al., EMNLP 2024
PDF: https://aclanthology.org/2024.emnlp-main.1235.pdf


24. Defensive Prompt Patch
Chen Xiong et al., May 30, 2024
PDF: https://arxiv.org/abs/2405.20099


25. Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs
NeurIPS 2024
PDF: https://papers.nips.cc/paper_files/paper/2024/file/38c1dfb4f7625907b15e9515365e7803-Paper-Datasets_and_Benchmarks_Track.pdf


26. Rapid Response: Mitigating LLM Jailbreaks with a Few Examples
ArXiv preprint, late 2024
PDF: https://arxiv.org/html/2411.07494v1


27. Bypassing Meta’s LLaMA Classifier: A Simple Jailbreak
Aman Priyanshu; Cisco Blog, July 29, 2024
Blog: https://blogs.cisco.com/security/bypassing-metas-llama-classifier-a-simple-jailbreak


28. Some Notes on Adversarial Attacks on LLMs
Cybernetist Blog, Sep 23, 2024
Blog: https://cybernetist.com/2024/09/23/some-notes-on-adversarial-attacks-on-llms


29. Automated Jailbreaking of Large Language Model Chatbots
NDSS Symposium 2024
PDF: https://www.ndss-symposium.org/wp-content/uploads/2024-188-paper.pdf

2023

30. “Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models
Xinyue Shen et al., Aug 7, 2023
PDF: https://arxiv.org/abs/2308.03825


31. Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study
Anonymous, May 2023
PDF: https://arxiv.org/pdf/2305.13860


32. Universal and Transferable Adversarial Attacks on Aligned Language Models
Andy Zou et al., Jul 27, 2023
PDF: https://arxiv.org/abs/2307.15043
GitHub: https://github.com/llm-attacks/llm-attacks


33. AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models
Sicheng Zhu et al., Oct 23, 2023
PDF: https://arxiv.org/abs/2310.15140


34. An LLM can Fool Itself: A Prompt-Based Adversarial Attack
Xilie Xu et al., Oct 20, 2023
PDF: https://arxiv.org/abs/2310.13345


35. GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts
Jiahao Yu et al., Sep 19, 2023
PDF: https://arxiv.org/abs/2309.10253
GitHub: https://github.com/sherdencooper/GPTFuzz


36. Jailbroken: How Does LLM Safety Training Fail?
Alexander Wei et al., Jul 2023
PDF: https://arxiv.org/abs/2307.02483


37. Bing Chat Prompt Injection ("Sydney" leak)
Kevin Liu, Feb 2023
Ars Technica: https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/
LiveMint: https://www.livemint.com/technology/stanford-student-cracks-microsoft-s-ai-powered-bing-chat-secrets-twice-details-11676355563678.html


38. Exclusive: IBM Researchers Easily Trick ChatGPT into Hacking
IBM X-Force via Axios, Aug 8, 2023
Axios: https://www.axios.com/2023/08/08/ibm-researchers-trick-chatgpt-hacking
